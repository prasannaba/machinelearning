{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron_2(object):\n",
    "    \"\"\"\n",
    "    Inputs: eta & maxiter\n",
    "    Output: object\n",
    "    \"\"\"\n",
    "    def __init__(self, eta, maxiter) -> None:\n",
    "        self.eta = eta\n",
    "        self.maxiter = maxiter\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "            Perceptron\n",
    "            The activation function here is unit step function\n",
    "        \"\"\"\n",
    "        self.w_ = np.zeros(1+X.shape[1])\n",
    "        self.errors_ = []\n",
    "        self.iterations = 0\n",
    "        errors = 1\n",
    "        while (self.iterations < self.maxiter and errors!=0):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_[1:] += update * xi\n",
    "                self.w_[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.iterations += 1\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "    def fit_sgd(self, X, y):\n",
    "        \"\"\"\n",
    "            Perceptron using Stochastic Gradient Descent.\n",
    "            L(w, b) = -y(w.x+b) is for each point\n",
    "\n",
    "            Keep in mind this is a customized loss function based on loss on \n",
    "            each point & in SGD we update w & b based on each point. We can also\n",
    "            use on the shelf loss functions like least square,logistic, unit step\n",
    "            .... using both gradient & stochastic descent.\n",
    "\n",
    "            How we arrived at Loss Function:\n",
    "            Input Data Background:\n",
    "                X = [x1, x2, x3,....xd] d-dimensional with each dimension being\n",
    "                vector of real numbers..\n",
    "                y = -1 or +1\n",
    "\n",
    "                In euclidean space X = [x1, x2], a point. So in this case the \n",
    "                data sample of X & y looks like \n",
    "                X = [[1, 2], [3, 4], [5, 3],....], \n",
    "                y=[-1, +1, -1,....]\n",
    "            \n",
    "            Algorithm:\n",
    "            From X = [x1, x2, x3,...] the linear classifier is estimated \n",
    "            from the function: w1x1 + w2x2 +.....+b = w.x+b\n",
    "\n",
    "            w.X+b = 0 is the decision boundary\n",
    "            w.X+b>0 point is above the decision boundary & it is classified as y = +1\n",
    "            w.X+b<=0 point is below the decision boundary & it is classified as y =-1\n",
    "\n",
    "            Goal is to find w, b based on training data of X & y\n",
    "\n",
    "            Consider based on current values of w & b for sample point x\n",
    "            y=-1, w.x+b=-c then y(w.x+b)=c >0, where c is some number\n",
    "            y=1, w.x+b=c then y(w.x+b)=c >0\n",
    "            In both the cases we are right on the target & prediction for point x\n",
    "            i.e y is positive & sign of w.x+b is positive & vice-versa so\n",
    "            if y(w.x+b)>0, we don't have to anything, the point is classified correctly\n",
    "\n",
    "            Now consider,\n",
    "            y=-1, w.x+b=c, y(w.x+b)=-c <= 0, where c is some number\n",
    "            y=1, w.x+b=-c, y(w.x+b)=-c <= 0\n",
    "            so we are wrong on a point x, if y(w.x+b)<=0\n",
    "            and by being wrong, how much we are losing? \n",
    "            We are losing by amount c on each wrongly classified point\n",
    "            so rearranging y(w.X+b)=-c, \n",
    "            Loss c = -y(w.x+b), which is function of w & b, & we want to minimize it by \n",
    "            finding the optimal value of w & b\n",
    "            w & b are updated only when we are wrong on the point that is \n",
    "            y(w.x+b)<=0 because we are wrong on classified point & there is loss c\n",
    "            \n",
    "            So the loss function is:\n",
    "                L(w, b) = -y(w.x+b)\n",
    "\n",
    "            Derivative of Loss(w, b) with respect to w & b gives\n",
    "            \n",
    "            dL/dw = -yx\n",
    "            dL/db = -y\n",
    "\n",
    "            Stochastic Gradient Descent till convergence:\n",
    "\n",
    "            w = w - (-yx) \n",
    "            w = w+yx\n",
    "\n",
    "            b = b - (-y)\n",
    "            b = b+y\n",
    "            \n",
    "        \"\"\"\n",
    "        self.w_=np.zeros(1+X.shape[1])\n",
    "        self.errors_ = []\n",
    "        self.iterations = 0\n",
    "        errors = 1\n",
    "        while (self.iterations < self.maxiter and errors!=0):\n",
    "            errors = 0\n",
    "            X, y = self.shuffle(X, y)\n",
    "            for xi, target in zip(X, y):\n",
    "                if (target * self.predict(xi)) <= 0:\n",
    "                    self.w_[1:] += self.eta * target * xi\n",
    "                    self.w_[0] += self.eta * target\n",
    "                    errors += 1\n",
    "            self.iterations += 1\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "    def fit_sgd_one(self, X, y):\n",
    "        \"\"\"\n",
    "            L(w, b)= Sum(y-(wX+b)**2)\n",
    "\n",
    "            derivative \n",
    "            dL/dw = -sum((y-(w.X+b))X)\n",
    "            db/dw = -sum(y-(w.X+b))\n",
    "\n",
    "            In SGD we calculate on each point, implementation reflects this.\n",
    "        \"\"\"\n",
    "        self.w_=np.zeros(1+X.shape[1])\n",
    "        self.errors_ = []\n",
    "        self.iterations = 0\n",
    "        errors = 1\n",
    "        diff_errors = 1\n",
    "        while (self.iterations < self.maxiter and errors!=0):\n",
    "            errors=0\n",
    "            X, y = self.shuffle(X, y)\n",
    "            for x1, y1 in zip(X, y):\n",
    "                self.w_[1:] += self.eta * x1.dot((y1-self.predict(x1)))\n",
    "                self.w_[0] += self.eta  * (y1-self.predict(x1))\n",
    "                errors += (y1-self.predict(x1))**2\n",
    "            self.iterations += 1\n",
    "            self.errors_.append(errors)\n",
    "            diff_errors = abs(diff_errors-errors)\n",
    "        # print(self.iterations)\n",
    "        return self\n",
    "    \n",
    "    def fit_gd(self, X, y):\n",
    "        \"\"\"\n",
    "            In case of Gradient Descent, we act on the complete data set to \n",
    "            calculate single instance of w & b.\n",
    "            So Loss function need to be defined keeping in mind the that we are\n",
    "            going to act on complete data set, hence summation is required over \n",
    "            loss on each point. Here we are using simple loss function\n",
    "            L(w, b)= sum(y-(wX+b)**2)\n",
    "\n",
    "            derivative \n",
    "            dL/dw = -sum((y-(w.X+b))X)\n",
    "            db/dw = -sum(y-(w.X+b))\n",
    "        \"\"\"\n",
    "        self.w_=np.zeros(1+X.shape[1])\n",
    "        self.errors_ = []\n",
    "        self.iterations = 0\n",
    "        errors = 1\n",
    "        while (self.iterations < self.maxiter and errors!=0):\n",
    "            self.w_[1:] += self.eta * X.T.dot((y-self.predict(X)))\n",
    "            self.w_[0] += self.eta * (y-self.predict(X)).sum()\n",
    "            errors = ((y-self.predict(X))**2).sum()\n",
    "            self.iterations += 1\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"\n",
    "            Returns dot product of w.x + b\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.w_[1:])+self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            Returns the sign of w.x + b\n",
    "        \"\"\"\n",
    "        return np.where(self.net_input(X)> 0, 1, -1)\n",
    "\n",
    "    def shuffle(self, X, y):\n",
    "        i = np.random.permutation(len(X))\n",
    "        return(X[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,2], [1, 4], [1, 7], [2, 5], [2, 8], [2, 6], [3, 6], [3, 9], [4, 2], [2, 10],[10, 8]])\n",
    "y = np.array([-1, -1, 1, -1, 1, 1, 1, 1, -1, 1, 1])\n",
    "# plt.scatter(x[:, 0], x[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prn = Perceptron_2(eta=0.01, maxiter=100)\n",
    "prn.fit_gd(x, y)\n",
    "print(f'Total number of iterations: {prn.iterations}')\n",
    "plt.plot(range(len(prn.errors_)), prn.errors_)\n",
    "print(f'Errors: {prn.errors_}')\n",
    "print(f'Intercept & Co-efficients: {prn.w_}')\n",
    "test = np.array([[1, 4], [4, 4], [6, 7], [5, 4], [1, 4.9], [10, 10], [10, 3]])\n",
    "print(f'Predicted values: {prn.predict(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prn = Perceptron_2(eta=0.01, maxiter=1000)\n",
    "prn.fit_sgd(x, y)\n",
    "print(f'Total number of iterations: {prn.iterations}')\n",
    "plt.plot(range(len(prn.errors_)), prn.errors_)\n",
    "print(f'Errors: {prn.errors_}')\n",
    "print(f'Intercept & Co-efficients: {prn.w_}')\n",
    "test = np.array([[1, 4], [4, 4], [6, 7], [5, 4], [1, 4.9], [10, 10], [10, 3]])\n",
    "print(f'Predicted values: {prn.predict(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prn = Perceptron_2(eta=0.001, maxiter=100)\n",
    "prn.fit_sgd_one(x, y)\n",
    "print(f'Total number of iterations: {prn.iterations}')\n",
    "plt.plot(range(len(prn.errors_)), prn.errors_)\n",
    "print(f'Errors: {prn.errors_}')\n",
    "print(f'Intercept & Co-efficients: {prn.w_}')\n",
    "test = np.array([[1, 4], [4, 4], [6, 7], [5, 4], [1, 4.9], [10, 10], [10, 3]])\n",
    "print(f'Predicted values: {prn.predict(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prn = Perceptron_2(eta=0.1, maxiter=100)\n",
    "prn.fit(x, y)\n",
    "print(f'Total number of iterations: {prn.iterations}')\n",
    "plt.plot(range(len(prn.errors_)), prn.errors_)\n",
    "print(f'Errors: {prn.errors_}')\n",
    "print(f'Intercept & Co-efficients: {prn.w_}')\n",
    "test = np.array([[1, 4], [4, 4], [6, 7], [5, 4], [1, 4.9], [10, 10], [10, 3]])\n",
    "print(f'Predicted values: {prn.predict(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sklearn Perceptron, keep in mind we have to use StandardScaler()\n",
    "sc = StandardScaler()\n",
    "sc.fit(x)\n",
    "x_p = sc.transform(x)\n",
    "x_t = sc.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Perceptron(eta0=0.1, max_iter=100, random_state=0)\n",
    "clf.fit(x_p, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
